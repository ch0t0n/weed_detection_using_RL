{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(389, {'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('Taxi-v3', render_mode='human')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mget()\n\u001b[0;32m      5\u001b[0m action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()  \u001b[38;5;66;03m# this is where you would insert your policy\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[0;32m      9\u001b[0m    observation, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32mc:\\Users\\choto\\miniconda3\\envs\\weed\\Lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\choto\\miniconda3\\envs\\weed\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\choto\\miniconda3\\envs\\weed\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\choto\\miniconda3\\envs\\weed\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\taxi.py:293\u001b[0m, in \u001b[0;36mTaxiEnv.step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlastaction \u001b[38;5;241m=\u001b[39m a\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mint\u001b[39m(s), r, t, \u001b[38;5;28;01mFalse\u001b[39;00m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m\"\u001b[39m: p, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_mask(s)})\n",
      "File \u001b[1;32mc:\\Users\\choto\\miniconda3\\envs\\weed\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\taxi.py:323\u001b[0m, in \u001b[0;36mTaxiEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_gui(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode)\n",
      "File \u001b[1;32mc:\\Users\\choto\\miniconda3\\envs\\weed\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\taxi.py:449\u001b[0m, in \u001b[0;36mTaxiEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    448\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[0;32m    452\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    453\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "observation, info = env.reset()\n",
    "for _ in range(1000):\n",
    "   pygame.event.get()\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode=None, size=5):\n",
    "        self.size = size  # The size of the square grid\n",
    "        self.window_size = 512  # The size of the PyGame window\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent1\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"agent2\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"target\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = spaces.MultiDiscrete([4,4])\n",
    "\n",
    "        \"\"\"\n",
    "        The following dictionary maps abstract actions from `self.action_space` to\n",
    "        the direction we will walk in if that action is taken.\n",
    "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "        \"\"\"\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),\n",
    "            1: np.array([0, 1]),\n",
    "            2: np.array([-1, 0]),\n",
    "            3: np.array([0, -1]),\n",
    "        }\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\"agent1\": self._agent_location,  \"target\": self._target_location}\n",
    "    \n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._agent_location - self._target_location, ord=1\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Choose the agent's location uniformly at random\n",
    "        self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "\n",
    "        # We will sample the target's location randomly until it does not coincide with the agent's location\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._target_location, self._agent_location):\n",
    "            self._target_location = self.np_random.integers(\n",
    "                0, self.size, size=2, dtype=int\n",
    "            )\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "        direction = self._action_to_direction[action]\n",
    "        # We use `np.clip` to make sure we don't leave the grid\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1\n",
    "        )\n",
    "        # An episode is done iff the agent has reached the target\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        reward = 1 if terminated else 0  # Binary sparse rewards\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "    \n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode(\n",
    "                (self.window_size, self.window_size)\n",
    "            )\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # First we draw the target\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._target_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        # Now we draw the agent\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),\n",
    "            (self._agent_location + 0.5) * pix_square_size,\n",
    "            pix_square_size / 3,\n",
    "        )\n",
    "\n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,\n",
    "            )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "class TwoAgentGridworldEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human', 'print'], \"render_fps\": 4}\n",
    "    \n",
    "    def __init__(self, render_mode=None, grid_size=(10, 10)):\n",
    "        super(TwoAgentGridworldEnv, self).__init__()\n",
    "        self.size = grid_size[0]  # The size of the square grid\n",
    "        self.window_size = 800  # The size of the PyGame window\n",
    "        \n",
    "        self.grid_size = grid_size\n",
    "        self.action_space = spaces.MultiDiscrete([4, 4])  # 4 possible actions for each of the two agents\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Box(low=0, high=grid_size[0]-1, shape=(2,), dtype=np.int32),\n",
    "            spaces.Box(low=0, high=grid_size[1]-1, shape=(2,), dtype=np.int32)\n",
    "        ))\n",
    "        \n",
    "        \n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        # Reset the environment and start\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_positions = [\n",
    "            np.array([0, 0]),  # Agent 1 starts at top-left corner\n",
    "            np.array([self.grid_size[0]-1, self.grid_size[1]-1])  # Agent 2 starts at bottom-right corner\n",
    "        ]\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {'agent1': self.agent_positions[0], 'agent2': self.agent_positions[1]}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Define the movements corresponding to each action\n",
    "        movements = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # up, down, left, right\n",
    "        rewards = 0\n",
    "        \n",
    "        # Update the positions of both agents\n",
    "        for i, act in enumerate(action):\n",
    "            movement = movements[act]\n",
    "            new_position = self.agent_positions[i] + movement\n",
    "            # Ensure the new position is within bounds\n",
    "            new_position = np.clip(new_position, [0, 0], [self.grid_size[0]-1, self.grid_size[1]-1])\n",
    "            self.agent_positions[i] = new_position\n",
    "        \n",
    "        # Placeholder for terminal state and rewards\n",
    "        done = False\n",
    "        reward = 0\n",
    "        \n",
    "        # If the agents meet at the same position, we can assign a reward or consider it a terminal state\n",
    "        if np.array_equal(self.agent_positions[0], self.agent_positions[1]):\n",
    "            done = True\n",
    "            reward = 10  # Example reward for meeting at the same position\n",
    "        \n",
    "        obs = self._get_obs()\n",
    "        return obs, reward, done, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == 'print':\n",
    "            grid = np.zeros(self.grid_size)\n",
    "            grid[tuple(self.agent_positions[0])] = 1  # Mark the position of the first agent\n",
    "            grid[tuple(self.agent_positions[1])] = 2  # Mark the position of the second agent\n",
    "            print(grid)\n",
    "        elif self.render_mode == 'human':\n",
    "            if self.window is None: # Initialize pygame if it is not initialized\n",
    "                pygame.init()\n",
    "                pygame.display.init()\n",
    "                self.window = pygame.display.set_mode(\n",
    "                    (self.window_size, self.window_size)\n",
    "                )\n",
    "            if self.clock is None:\n",
    "                self.clock = pygame.time.Clock()\n",
    "            \n",
    "            # Fill the canvas\n",
    "            canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "            canvas.fill((255, 255, 255))\n",
    "            pix_square_size = (\n",
    "                self.window_size / self.size\n",
    "            )  # The size of a single grid square in pixels\n",
    "\n",
    "\n",
    "            # First we draw agent1 square\n",
    "            pygame.draw.rect(\n",
    "                canvas,\n",
    "                (255, 0, 0),\n",
    "                pygame.Rect(\n",
    "                    pix_square_size * self.agent_positions[0],\n",
    "                    (pix_square_size, pix_square_size),\n",
    "                ),\n",
    "            )\n",
    "            # Now we draw the agent2 circle\n",
    "            pygame.draw.circle(\n",
    "                canvas,\n",
    "                (0, 0, 255),\n",
    "                (self.agent_positions[1] + 0.5) * pix_square_size,\n",
    "                pix_square_size / 3,\n",
    "            )\n",
    "            # Finally, add some gridlines\n",
    "            for x in range(self.size + 1):\n",
    "                pygame.draw.line(\n",
    "                    canvas,\n",
    "                    0,\n",
    "                    (0, pix_square_size * x),\n",
    "                    (self.window_size, pix_square_size * x),\n",
    "                    width=3,\n",
    "                )\n",
    "                pygame.draw.line(\n",
    "                    canvas,\n",
    "                    0,\n",
    "                    (pix_square_size * x, 0),\n",
    "                    (pix_square_size * x, self.window_size),\n",
    "                    width=3,\n",
    "                )\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "            \n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs: {'agent1': array([0, 1]), 'agent2': array([9, 9])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([1, 1]), 'agent2': array([8, 9])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([1, 2]), 'agent2': array([7, 9])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([0, 2]), 'agent2': array([7, 9])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([1, 2]), 'agent2': array([7, 8])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([1, 1]), 'agent2': array([8, 8])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([2, 1]), 'agent2': array([7, 8])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([1, 1]), 'agent2': array([8, 8])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([0, 1]), 'agent2': array([8, 9])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([1, 1]), 'agent2': array([8, 9])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([1, 0]), 'agent2': array([9, 9])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([2, 0]), 'agent2': array([9, 9])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([3, 0]), 'agent2': array([9, 9])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([3, 1]), 'agent2': array([9, 9])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([3, 2]), 'agent2': array([9, 8])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([2, 2]), 'agent2': array([9, 7])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([2, 3]), 'agent2': array([9, 6])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([2, 2]), 'agent2': array([9, 5])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([2, 1]), 'agent2': array([8, 5])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([1, 1]), 'agent2': array([9, 5])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([2, 1]), 'agent2': array([9, 6])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([2, 0]), 'agent2': array([9, 6])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([2, 0]), 'agent2': array([8, 6])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([3, 0]), 'agent2': array([8, 5])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([3, 1]), 'agent2': array([8, 4])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([3, 0]), 'agent2': array([9, 4])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([4, 0]), 'agent2': array([9, 5])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([3, 0]), 'agent2': array([8, 5])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([3, 0]), 'agent2': array([8, 4])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([3, 0]), 'agent2': array([9, 4])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([3, 0]), 'agent2': array([9, 3])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([3, 0]), 'agent2': array([8, 3])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([3, 0]), 'agent2': array([8, 2])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([4, 0]), 'agent2': array([7, 2])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([4, 0]), 'agent2': array([8, 2])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([4, 0]), 'agent2': array([9, 2])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([3, 0]), 'agent2': array([9, 3])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([4, 0]), 'agent2': array([9, 4])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([4, 1]), 'agent2': array([9, 3])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([5, 1]), 'agent2': array([8, 3])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([5, 0]), 'agent2': array([8, 2])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([6, 0]), 'agent2': array([9, 2])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([5, 0]), 'agent2': array([8, 2])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([6, 0]), 'agent2': array([9, 2])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([6, 0]), 'agent2': array([9, 1])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([6, 1]), 'agent2': array([9, 0])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([5, 1]), 'agent2': array([8, 0])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([5, 2]), 'agent2': array([8, 0])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([4, 2]), 'agent2': array([8, 1])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([4, 3]), 'agent2': array([7, 1])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([3, 3]), 'agent2': array([6, 1])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([2, 3]), 'agent2': array([6, 0])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([2, 4]), 'agent2': array([6, 0])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([1, 4]), 'agent2': array([7, 0])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([1, 3]), 'agent2': array([7, 0])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([2, 3]), 'agent2': array([6, 0])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([2, 2]), 'agent2': array([5, 0])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([2, 3]), 'agent2': array([5, 0])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([2, 4]), 'agent2': array([5, 0])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([2, 3]), 'agent2': array([5, 1])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([2, 4]), 'agent2': array([5, 2])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([3, 4]), 'agent2': array([4, 2])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([2, 4]), 'agent2': array([4, 3])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([2, 3]), 'agent2': array([4, 2])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([1, 3]), 'agent2': array([3, 2])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([0, 3]), 'agent2': array([3, 3])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([0, 2]), 'agent2': array([2, 3])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([0, 2]), 'agent2': array([3, 3])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([0, 3]), 'agent2': array([4, 3])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([0, 4]), 'agent2': array([5, 3])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([0, 5]), 'agent2': array([4, 3])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([0, 4]), 'agent2': array([4, 4])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([1, 4]), 'agent2': array([3, 4])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([2, 4]), 'agent2': array([3, 3])}, Reward: 0, Done: False\n",
      "Obs: {'agent1': array([3, 4]), 'agent2': array([3, 4])}, Reward: 10, Done: True\n"
     ]
    }
   ],
   "source": [
    "# Register the environment\n",
    "gym.envs.registration.register(\n",
    "    id='TwoAgentGridworld-v0',\n",
    "    entry_point=TwoAgentGridworldEnv,\n",
    "    max_episode_steps=100,\n",
    ")\n",
    "\n",
    "# Example of creating and using the environment\n",
    "env = gym.make('TwoAgentGridworld-v0', render_mode='human')\n",
    "env.metadata['render_fps'] = 8\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, _,  info = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"Obs: {obs}, Reward: {reward}, Done: {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
