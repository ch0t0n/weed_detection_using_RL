{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************** This is the best working model so far ***********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muti-agent reinforcement learning for Precision Agriculture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "# Import libraries\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "import pygame\n",
    "from gymnasium import spaces\n",
    "from shapely import Polygon\n",
    "from shapely.geometry import Point\n",
    "import yaml\n",
    "import copy\n",
    "np.random.seed(33) # seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 \n",
    "# Read the experiments file and select the experiment\n",
    "experiment_set_path = r'experiments/set1.yaml'\n",
    "with open(experiment_set_path, 'r') as experiment_file:\n",
    "    config = yaml.load(experiment_file, Loader=yaml.FullLoader)\n",
    "    config['field'] = list(map(lambda x: tuple(x), config['field']))\n",
    "    config['init_positions'] = list(map(lambda x: np.array(x), config['init_positions']))\n",
    "    config['infected_locations'] = list(map(lambda x: tuple(x), config['infected_locations']))\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helpful functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "# function for random starting points\n",
    "# unused\n",
    "def random_starting_locations(points, infected_locations):\n",
    "    points = set([tuple(p) for p in points])\n",
    "    st_locs = points.difference(infected_locations) # for points - infected_locations\n",
    "    st_locs = random.choices(np.array(list(st_locs)),k=3)\n",
    "    return st_locs\n",
    "\n",
    "# convert binary list to decimal\n",
    "def binary_list_to_decimal(bin_list):\n",
    "    bin = ''\n",
    "    for b in bin_list:\n",
    "        bin += str(b)\n",
    "    dec = int(bin,2)\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the multi-agent class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "class ThreeAgentGridworldEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human', 'print', 'rgb_array'], \"render_fps\": 4}    \n",
    "    def __init__(self, render_mode=None, grid_size=(50, 50), env_config=config):\n",
    "        super(ThreeAgentGridworldEnv, self).__init__()\n",
    "        self.config = copy.deepcopy(env_config)\n",
    "        self.poly_vertices = self.config['field']\n",
    "        self.Poly = Polygon(self.poly_vertices) # Get the points of the polygon\n",
    "        self.size = grid_size[0]  # The size of the square grid\n",
    "        self.window_size = 800  # The size of the PyGame window        \n",
    "        self.grid_size = grid_size # Size of the grid (May need to remove later on)\n",
    "        self.outer_boundary = self.Poly.buffer(distance=2) # outer boundary with buffer of distance 2\n",
    "\n",
    "        # Observation points\n",
    "        self.observation_points = self.obs_points()\n",
    "        # print('Observation points:', self.observation_points)\n",
    "        self.observation_length = len(self.observation_points)\n",
    "        self.observation_map = {tuple(v):i for i,v in enumerate(self.observation_points)}\n",
    "        # for p in self.observation_points:\n",
    "        #     print('observation map:', p, self.observation_map[tuple(p)])\n",
    "\n",
    "        # Keep track of visited states and count steps\n",
    "        self.step_count = 0\n",
    "        self.visited = set()\n",
    "        self.infected_locations = self.config['infected_locations']\n",
    "        \n",
    "        self.infected_length = len(self.infected_locations)\n",
    "        self.infected_state_length = 2**(len(self.infected_locations)) # 2**5, binary to decimal\n",
    "        self.infected_dict = {v:0 for v in self.infected_locations} # dictionary of locations\n",
    "        \n",
    "        # Action and observation space\n",
    "        self.action_space = spaces.MultiDiscrete([5, 5, 5])  # 4 possible actions for each of the two agents\n",
    "        # self.observation_positions = spaces.MultiDiscrete([self.observation_length, self.observation_length, self.observation_length])\n",
    "        # self.infected_space = spaces.MultiBinary(5)\n",
    "        # self.observation_space = spaces.Tuple((self.observation_positions, self.infected_space))\n",
    "        self.observation_space = spaces.MultiDiscrete([self.observation_length, self.observation_length, self.observation_length, self.infected_state_length])\n",
    "        \n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"] # Check if the render mode is correct\n",
    "        self.render_mode = render_mode\n",
    "        # If human-rendering is used, `self.window` will be a reference\n",
    "        # to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        # to ensure that the environment is rendered at the correct framerate in\n",
    "        # human-mode. They will remain `None` until human-mode is used for the\n",
    "        # first time.\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "        # Reset the environment and start\n",
    "        self.reset()\n",
    "\n",
    "    def obs_points(self):\n",
    "        xp,yp = self.Poly.exterior.xy\n",
    "        # Gridpoints\n",
    "        xs = np.arange(0, 90, 1)\n",
    "        ys = np.arange(0, 90, 1)\n",
    "        # Inside points\n",
    "        # print(xs,ys)\n",
    "        xps, yps = [], []\n",
    "        for xi in xs:\n",
    "            for yi in ys:\n",
    "                p = Point(xi,yi)\n",
    "                if self.Poly.contains(p):\n",
    "                    xps += [p.x]\n",
    "                    yps += [p.y]\n",
    "        xps += xp\n",
    "        yps += yp\n",
    "        # plt.plot(xp,yp)\n",
    "        # plt.scatter(xps,yps, color='r')\n",
    "        obs_points = np.array(list(set(zip(xps,yps)))) # Taking unique observation points\n",
    "        return obs_points\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        a1, a2, a3 = self.agent_positions[0], self.agent_positions[1], self.agent_positions[2]\n",
    "        info = {'agent1': a1, 'agent2': a2, 'agent3': a3, 'step_count': self.step_count}\n",
    "        # print('agent positions:', a1, a2, a3)\n",
    "        p1,p2,p3 = self.observation_map[tuple(a1)], self.observation_map[tuple(a2)], self.observation_map[tuple(a3)]\n",
    "        infected = binary_list_to_decimal(list(self.infected_dict.values()))\n",
    "        state = np.array([p1,p2,p3,infected]) # convert the infected binary list to decimal\n",
    "        return state, info\n",
    "\n",
    "    def reset(self, seed=None, options={}):\n",
    "        self.visited = set()\n",
    "        self.step_count = 0\n",
    "        self.infected_locations = copy.copy(self.config['infected_locations'])\n",
    "        self.infected_dict = {v:0 for v in self.infected_locations} # dictionary of locations\n",
    "        self.agent_positions = self.config['init_positions']\n",
    "        # self.agent_positions = [\n",
    "        #     np.array([14, 34]),  # Agent 1 \n",
    "        #     # np.array([self.grid_size[0]-1, self.grid_size[1]-1]),  # Agent 2 starts at bottom-right corner,\n",
    "        #     # np.array([self.grid_size[0]-3, self.grid_size[1]-3])  # Agent 3 starts at specific position,\n",
    "        #     np.array([80/2, 50/2]),  # Agent 2 \n",
    "        #     np.array([40/2, 80/2]),  # Agent 3\n",
    "        # ]\n",
    "        # self.agent_positions = random_starting_locations(self.observation_points, self.infected_locations)\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Placeholder for terminal state and rewards\n",
    "        terminated, truncated = False, False\n",
    "        rewards = 0\n",
    "        self.step_count += 1\n",
    "\n",
    "        # Define the movements corresponding to each action\n",
    "        movements = [(-1, 0), (1, 0), (0, -1), (0, 1), (0, 0)]  # up, down, left, right, none\n",
    "        \n",
    "        # Update the positions of both agents\n",
    "        for i, act in enumerate(action):\n",
    "            # if act == 4: # If the action is none\n",
    "            #     terminated = True # Terminated\n",
    "            #     break\n",
    "\n",
    "            movement = movements[act] # What movement to take\n",
    "            new_position = self.agent_positions[i] + movement # New position after movement\n",
    "            \n",
    "            # Ensure the new position is within bounds\n",
    "            # new_position = np.clip(new_position, [0, 0], [self.grid_size[0]-1, self.grid_size[1]-1])\n",
    "            new_p = Point(new_position[0], new_position[1])\n",
    "            # print(\"checking new position\", tuple(new_position))\n",
    "            if self.Poly.contains(new_p):\n",
    "                self.agent_positions[i] = new_position\n",
    "            else:\n",
    "                rewards -= 10\n",
    "            if tuple(new_position) in self.visited:\n",
    "                rewards -= 10\n",
    "            else:\n",
    "                rewards -= 1\n",
    "            self.visited.add(tuple(new_position))\n",
    "        \n",
    "        # Check if an infected location is visited\n",
    "        # JW: Should we make a dedicated action for removing the weed instead of doing it automatically?\n",
    "        # JW: Perhaps adding a cost of removing the weed since real drones will have limited herbicide and should be discouraged from wasting it\n",
    "        infected_visited = [x for x in self.agent_positions if tuple(x) in self.infected_locations] # If infected cells are visited\n",
    "        for v in infected_visited:\n",
    "            if tuple(v) in self.infected_locations:\n",
    "                self.infected_locations.remove(tuple(v))\n",
    "                self.infected_dict[tuple(v)] = 1\n",
    "                if len(self.infected_locations) == 0:\n",
    "                    print('Infected locations empty')\n",
    "        \n",
    "        if infected_visited:\n",
    "            rewards += 100 * len(infected_visited) \n",
    "\n",
    "        if sum(list(self.infected_dict.values()))==self.infected_length:\n",
    "            print('I cleared all the infected locations')\n",
    "            rewards += 100000\n",
    "            terminated = True\n",
    "\n",
    "        \n",
    "        # If the agents meet at the same position, we can assign a reward or consider it a terminal state\n",
    "        if np.array_equal(self.agent_positions[0], self.agent_positions[1]) or np.array_equal(self.agent_positions[0], self.agent_positions[2]) or np.array_equal(self.agent_positions[1], self.agent_positions[2]):\n",
    "            rewards -= 100000  # Infinity reward for meeting at the same position\n",
    "            terminated = True\n",
    "        \n",
    "        obs, info = self._get_obs()\n",
    "        # rewards = rewards * self.gamma ** self.step_count\n",
    "        return obs, rewards, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == 'print':\n",
    "            grid = np.zeros(self.grid_size)\n",
    "            grid[tuple(self.agent_positions[0])] = 1  # Mark the position of the first agent\n",
    "            grid[tuple(self.agent_positions[1])] = 2  # Mark the position of the second agent\n",
    "            print(grid)\n",
    "        else:\n",
    "            if self.window is None and self.render_mode == \"human\": # Initialize pygame if it is not initialized\n",
    "                pygame.init()\n",
    "                pygame.display.init()\n",
    "                self.window = pygame.display.set_mode(\n",
    "                    (self.window_size, self.window_size)\n",
    "                )\n",
    "            if self.clock is None and self.render_mode == \"human\":\n",
    "                self.clock = pygame.time.Clock()\n",
    "            \n",
    "            # Fill the canvas\n",
    "            canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "            canvas.fill((255, 255, 255))\n",
    "            pix_square_size = (\n",
    "                self.window_size / self.size\n",
    "            )  # The size of a single grid square in pixels\n",
    "\n",
    "            # Draw the polygon\n",
    "            pixel_poly_vertices = [(point[0] * pix_square_size, point[1] * pix_square_size) for point in self.poly_vertices]\n",
    "            pygame.draw.polygon(surface=canvas, \n",
    "                                color=(255, 255, 0), \n",
    "                                points=pixel_poly_vertices)\n",
    "            \n",
    "            # Draw the visited regions\n",
    "            for p in self.visited:\n",
    "                pygame.draw.rect(\n",
    "                canvas,\n",
    "                pygame.Color(100, 100, 100, a=0.5),\n",
    "                pygame.Rect(\n",
    "                    pix_square_size * np.array(p),\n",
    "                    (pix_square_size, pix_square_size),\n",
    "                ),\n",
    "                )\n",
    "            # Draw agent1 (square)\n",
    "            pygame.draw.rect(\n",
    "                canvas,\n",
    "                (255, 0, 0),\n",
    "                pygame.Rect(\n",
    "                    pix_square_size * self.agent_positions[0],\n",
    "                    (pix_square_size, pix_square_size),\n",
    "                ),\n",
    "            )\n",
    "            # Draw agent2 (circle)\n",
    "            pygame.draw.circle(\n",
    "                canvas,\n",
    "                (0, 0, 255),\n",
    "                (self.agent_positions[1] + 0.5) * pix_square_size,\n",
    "                pix_square_size / 3,\n",
    "            )\n",
    "            # Draw agent3 (circle)\n",
    "            pygame.draw.circle(\n",
    "                canvas,\n",
    "                (0, 255, 0),\n",
    "                (self.agent_positions[2] + 0.5) * pix_square_size,\n",
    "                pix_square_size / 3,\n",
    "            )\n",
    "            # Draw infected locations\n",
    "            for l in self.infected_locations:\n",
    "                pygame.draw.rect(\n",
    "                    canvas,\n",
    "                    (0, 255, 255),\n",
    "                    pygame.Rect(\n",
    "                        pix_square_size * np.array(l),\n",
    "                        (pix_square_size, pix_square_size),\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "\n",
    "            if self.render_mode == \"human\":\n",
    "                # The following line copies our drawings from `canvas` to the visible window\n",
    "                self.window.blit(canvas, canvas.get_rect())\n",
    "                pygame.event.pump()\n",
    "                pygame.display.update()\n",
    "\n",
    "                # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "                # The following line will automatically add a delay to keep the framerate stable.\n",
    "                self.clock.tick(self.metadata[\"render_fps\"])\n",
    "                # Finally\n",
    "                pygame.event.get()\n",
    "\n",
    "            elif self.render_mode == 'rgb_array':  # rgb_array\n",
    "                return np.transpose(\n",
    "                    np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "                )\n",
    "            \n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "\n",
    "# Register the environment\n",
    "gym.envs.registration.register(\n",
    "    id='ThreeAgentGridworld-v0',\n",
    "    entry_point=ThreeAgentGridworldEnv,\n",
    "    max_episode_steps=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the environment and random play (Optional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "# Example of creating and using the environment\n",
    "env = gym.make('ThreeAgentGridworld-v0', render_mode='human')\n",
    "env.metadata['render_fps'] = 30\n",
    "obs, info = env.reset()\n",
    "env.render()\n",
    "\n",
    "# Random play\n",
    "env.reset()\n",
    "terminated, truncated = False, False\n",
    "total_rewards = 0\n",
    "while not (terminated or truncated):\n",
    "    action = env.action_space.sample()\n",
    "    print(action)\n",
    "    obs, reward, terminated, truncated,  info = env.step(action)\n",
    "    env.render()\n",
    "    total_rewards += reward\n",
    "    print(f\"Obs: {obs}, Reward: {reward}, terminated: {terminated}, total_rewards: {total_rewards}\")\n",
    "    pygame.event.get()\n",
    "print('terminated:', terminated, 'truncated:', truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "# Close the environment\n",
    "env.close()\n",
    "# Set a breakpoint, will give an error\n",
    "assert False, \"breakpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training cells!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage actor-critic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "# keep track of running time\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# Advantage Actor Critic\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "vec_env = make_vec_env('ThreeAgentGridworld-v0', n_envs=4)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", vec_env, verbose=1, tensorboard_log=\"./logs\", gamma=0.99)\n",
    "model.learn(total_timesteps=1000000)\n",
    "model.save(\"trained_models/multi_agent_a2c_w2\")\n",
    "del model, vec_env\n",
    "end = time.time()\n",
    "duration = end-start\n",
    "print(\"Time taken for training:\", duration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trust Region Policy Optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "# keep track of running time\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# Advantage Actor Critic\n",
    "from sb3_contrib import TRPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "vec_env = make_vec_env('ThreeAgentGridworld-v0', n_envs=4)\n",
    "\n",
    "model = TRPO(\"MlpPolicy\", vec_env, verbose=1, tensorboard_log=\"./logs\", gamma=0.99)\n",
    "model.learn(total_timesteps=1000000)\n",
    "model.save(\"trained_models/multi_agent_trpo_w2\")\n",
    "del model, TRPO, make_vec_env\n",
    "end = time.time()\n",
    "duration = end-start\n",
    "print(\"Time taken for training:\", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "# Proximal Policy Optimization\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Training cell\n",
    "vec_env = make_vec_env('ThreeAgentGridworld-v0', n_envs=4)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=1, tensorboard_log=\"./logs\", gamma=0.99)\n",
    "model.learn(total_timesteps=300000)\n",
    "model.save(\"trained_models/multi_agent_ppo_w1\")\n",
    "del model, PPO, make_vec_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "# Recurrent Proximal Policy Optimization\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Training cell\n",
    "vec_env = make_vec_env('ThreeAgentGridworld-v0', n_envs=4)\n",
    "\n",
    "model = RecurrentPPO(\"MlpLstmPolicy\", vec_env, verbose=1, tensorboard_log=\"./logs\", gamma=0.99)\n",
    "model.learn(total_timesteps=300000)\n",
    "model.save(\"trained_models/multi_agent_recurrent_ppo_w1\")\n",
    "del model, RecurrentPPO, make_vec_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load trained network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 \n",
    "import pygame\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C # For PPO, need to change this\n",
    "\n",
    "# Load trained network\n",
    "model = A2C.load(\"trained_models/multi_agent_a2c_w2.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play using trained network and default env (we can also use vector env):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "# Make the environment\n",
    "env = gym.make('ThreeAgentGridworld-v0', render_mode='human')\n",
    "env.metadata['render_fps'] = 30\n",
    "obs, info = env.reset()\n",
    "env.render()\n",
    "\n",
    "# Start playing\n",
    "terminated, truncated = False, False\n",
    "total_rewards = 0\n",
    "while not (terminated or truncated):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, terminated, truncated,  info = env.step(list(action))\n",
    "    env.render()\n",
    "    total_rewards += reward\n",
    "    print(f\"Obs: {obs}, Reward: {reward}, terminated: {terminated}, total_rewards: {total_rewards}\")\n",
    "    pygame.event.get()\n",
    "print('terminated:', terminated, 'truncated:', truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: open the scene file using CoppeliaSim robot simulator before running the below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12\n",
    "# Simulation class\n",
    "import time\n",
    "from coppeliasim_zmqremoteapi_client import RemoteAPIClient\n",
    "import numpy as np\n",
    "\n",
    "# Start the remote API client\n",
    "client = RemoteAPIClient()\n",
    "sim = client.getObject('sim')\n",
    "defaultIdleFps = sim.getInt32Param(sim.intparam_idle_fps)\n",
    "sim.setInt32Param(sim.intparam_idle_fps, 0)\n",
    "\n",
    "# Height of movement\n",
    "height = 0.35\n",
    "\n",
    "class Drone_simulator:    \n",
    "    def __init__(self, polygon, scaling_factor, height):\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.scaled_polygon = [(x/scaling_factor,y/scaling_factor) for (x,y) in polygon]\n",
    "        self.rounded_polygon = self.scaled_polygon + [self.scaled_polygon[0]]\n",
    "        self.color = [[255,0,0],[255,0,255],[0,0,255]]\n",
    "        self.edges_3d = self.calc_edges_3d()\n",
    "        self.height = height\n",
    "\n",
    "    def start_simulation(self):\n",
    "        self.trace_line = sim.addDrawingObject(sim.drawing_lines, 2, 0, -1, 9999, [255,0,0]) # red line\n",
    "        sim.startSimulation()\n",
    "        print('Program started')\n",
    "\n",
    "    def stop_simulation(self):\n",
    "        sim.removeDrawingObject(self.trace_line)\n",
    "        sim.stopSimulation()\n",
    "\n",
    "    def calc_edges_3d(self):  # To calculate the edges in the polygon\n",
    "        edges = []\n",
    "        for i in range(len(self.rounded_polygon) - 1):\n",
    "            edges.append([list(self.rounded_polygon[i]), list(self.rounded_polygon[i+1])])\n",
    "        return edges\n",
    "\n",
    "    def draw_field(self):\n",
    "        white = [255, 255, 255]\n",
    "        lineContainer = sim.addDrawingObject(sim.drawing_lines, 2, 0, -1, 9999, white)\n",
    "        for l in self.edges_3d: # Drawing the field with white lines\n",
    "            line = l[0] + [self.height] + l[1] + [self.height]\n",
    "            for j in range(len(line)):\n",
    "                if line[j] != self.height:\n",
    "                    line[j] = int(line[j])\n",
    "            # print(line)\n",
    "            sim.addDrawingObjectItem(lineContainer, line)\n",
    "\n",
    "    def set_agent_positions(self, k, info):\n",
    "        for i in range(k):\n",
    "            drone = '/Quadcopter['\n",
    "            obj_path = drone+str(i)+']'\n",
    "            objHandle = sim.getObject(obj_path)\n",
    "            print(np.append(info['agent'+str(i+1)],[self.height]))\n",
    "            x = info['agent'+str(i+1)]\n",
    "            x = [xi/self.scaling_factor for xi in x]\n",
    "            x = x + [self.height]\n",
    "            print(x)\n",
    "            sim.setObjectPosition(objHandle, -1, x) # Initiate the position of the robots\n",
    "    \n",
    "    def set_weed_locations(self, weed_locations):\n",
    "        weed_obj = sim.getObject('/weed')\n",
    "        for i, loc in enumerate(weed_locations):\n",
    "            new_weed_obj = sim.copyPasteObjects([weed_obj])[0]\n",
    "            x = [xi/self.scaling_factor for xi in loc]\n",
    "            new_pos = x + [0]\n",
    "            sim.setObjectPosition(new_weed_obj, -1, new_pos)\n",
    "\n",
    "    def move_agents(self, k, info):\n",
    "        for i in range(k):\n",
    "            obj_path = '/target[' + str(i) + ']'\n",
    "            objHandle = sim.getObject(obj_path)\n",
    "            prev_pos = sim.getObjectPosition(objHandle, -1) # current object position\n",
    "            print(np.append(info['agent'+str(i+1)],[self.height]))\n",
    "            x = info['agent'+str(i+1)] # Get the x,y from info of gym env\n",
    "            x = [xi/self.scaling_factor for xi in x] # scale the x,y\n",
    "            x = x + [self.height] # add the z (height)\n",
    "            # print(x)\n",
    "            sim.setObjectPosition(objHandle, -1, x) # Initiate the position of the robots\n",
    "            # draw the line\n",
    "            line_data = prev_pos + x\n",
    "            sim.addDrawingObjectItem(self.trace_line, line_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulation using random movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13\n",
    "# Make the environment\n",
    "env = gym.make('ThreeAgentGridworld-v0', render_mode='human')\n",
    "env.metadata['render_fps'] = 5\n",
    "obs, info = env.reset()\n",
    "env.render()\n",
    "\n",
    "drone_simulator = Drone_simulator(polygon=env.poly_vertices, scaling_factor=5, height=height)\n",
    "drone_simulator.draw_field()\n",
    "drone_simulator.set_agent_positions(k=3, info=info)\n",
    "drone_simulator.set_weed_locations(weed_locations=env.infected_locations)\n",
    "\n",
    "# Start the simulator\n",
    "drone_simulator.start_simulation()\n",
    "terminated, truncated = False, False\n",
    "total_rewards = 0\n",
    "while not (terminated or truncated):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated,  info = env.step(action)\n",
    "    env.render()\n",
    "    total_rewards += reward\n",
    "    print(f\"Obs: {obs}, Reward: {reward}, terminated: {terminated}, total_rewards: {total_rewards}, action: {action}\")\n",
    "    pygame.event.get()\n",
    "    drone_simulator.move_agents(k=3, info=info) # Simulate\n",
    "print('terminated:', terminated, 'truncated:', truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulation using trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14\n",
    "# Make the environment\n",
    "env = gym.make('ThreeAgentGridworld-v0', render_mode='human')\n",
    "env.metadata['render_fps'] = 5\n",
    "obs, info = env.reset()\n",
    "env.render()\n",
    "\n",
    "# Make the simulator object, draw the field, and set agent positions\n",
    "drone_simulator = Drone_simulator(polygon=env.poly_vertices, scaling_factor=5, height=height)\n",
    "drone_simulator.draw_field()\n",
    "drone_simulator.set_agent_positions(k=3, info=info)\n",
    "drone_simulator.set_weed_locations(weed_locations=env.infected_locations)\n",
    "\n",
    "# Start simulation\n",
    "drone_simulator.start_simulation()\n",
    "terminated, truncated = False, False\n",
    "total_rewards = 0\n",
    "while not (terminated or truncated):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, terminated, truncated,  info = env.step(list(action))\n",
    "    env.render()\n",
    "    total_rewards += reward\n",
    "    print(f\"Obs: {obs}, Reward: {reward}, terminated: {terminated}, total_rewards: {total_rewards}, action: {action}\")\n",
    "    pygame.event.get()\n",
    "    drone_simulator.move_agents(k=3, info=info) # Simulate\n",
    "print('terminated:', terminated, 'truncated:', truncated)\n",
    "# drone_simulator.stop_simulation()\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15\n",
    "drone_simulator.stop_simulation()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2lenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
